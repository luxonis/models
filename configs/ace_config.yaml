model:
  name: aceaquatec
  type:
  pretrained:

  backbone:
    name: EfficientRep
    params:
      channels_list: [64, 128, 256, 512, 1024]
      num_repeats: [1, 6, 12, 18, 6]
      depth_mul: 0.33
      width_mul: 0.33

  neck:
    name: RepPANNeck
    params:
      channels_list: [256, 128, 128, 256, 256, 512]
      num_repeats: [12, 12, 12, 12]
      depth_mul: 0.33
      width_mul: 0.33

  heads:
    - name: KeypointBboxHead
      params:
        n_classes: 1
        n_keypoints: 13
        original_in_shape: [1, 3, 128, 256]
      loss:
        name: KeypointBoxLoss
        params:
          kpt_weight: 0.5
          box_weight: 0.05
          obj_weight: 0.2
          kptv_weight: 0.7
        weight: 0
      metrics:
        - name: ObjectKeypointSimilarity
          params:
            num_keypoints: 13
            n_classes: 1
            original_in_shape: [1, 3, 128, 256]
      visualizers:
        - name: KeypointVisualizer
          params:
            n_keypoints: 13
            n_classes: 1
    - name: SegmentationHead
      params:
        n_classes: 1
        original_in_shape: [1, 3, 128, 256]
      loss:
        name: BCEWithLogitsLoss
        params:
        weight: 100
      metrics:
        - name: F1Score
          params:
            task: binary
        - name: JaccardIndex
          params:
            task: binary
      visualizers:
        - name: SegmentationVisualizer

trainer:
  accelerator: auto
  devices: auto
  strategy: auto
  num_sanity_val_steps: 0
  profiler: null
  verbose: True

inferer:
  dataset_view: val

logger:
  project_name: aceaquatec
  run_name: &model_name null
  save_directory: output
  is_tensorboard: True
  is_wandb: False
  wandb_entity: luxonis
  is_mlflow: False
  logged_hyperparams:
    - train.epochs
    - train.batch_size

dataset:
  team_id: luxonis_team_id
  dataset_id: 6522f183f97ca319fc22978f
  dataset_name: RW
  bucket_type: BucketType.INTERNAL
  bucket_storage: BucketStorage.LOCAL
  train_view: train
  val_view: train
  test_view: train

train:
  preprocessing:
    train_image_size: [&height 128, &width 256]
    keep_aspect_ratio: False
    train_rgb: True
    normalize:
      active: True
      params:
        mean: 0.3755
        std: 0.2204
    augmentations:

  batch_size: 32
  accumulate_grad_batches: 1
  epochs: &epochs 1
  num_workers: 8
  train_metrics_interval: -1
  validation_interval: 1
  num_log_images: 8
  skip_last_batch: True
  main_head_index: 0
  use_rich_text: True

  callbacks: # callback specific parameters (check PL docs)
    use_device_stats_monitor: False
    model_checkpoint:
      save_top_k: 3
    early_stopping:
      active: False
      monitor: val_loss
      mode: min
      patience: 3
      verbose: True
    export_on_finish:
      active: true

  optimizers:
    optimizer:
      name: SGD
      params:
        lr: 0.02
        momentum: 0.937
        nesterov: True
        weight_decay: 0.0005
    scheduler:
      name: CosineAnnealingLR
      params:
        T_max: *epochs
        eta_min: 0

  freeze_modules:
    backbone: False
    neck: False
    heads: [False, False]

  losses:
    log_sub_losses: True
