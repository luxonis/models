import io
from typing import Literal

import cv2
import matplotlib.pyplot as plt
import numpy as np
import numpy.typing as npt
import torch
import torchvision.transforms.functional as F
import torchvision.transforms.functional as TF
from matplotlib.figure import Figure
from PIL import Image
from torch import Tensor
from torchvision.ops import box_convert
from torchvision.utils import (
    draw_bounding_boxes,
    draw_keypoints,
    draw_segmentation_masks,
)

from luxonis_train.utils.config import Config

Color = str | tuple[int, int, int]


def figure_to_torch(fig: Figure, width: int, height: int) -> Tensor:
    """Converts a matplotlib `Figure` to a `Tensor`."""
    buf = io.BytesIO()
    fig.savefig(buf, format="png", bbox_inches="tight", pad_inches=0)
    buf.seek(0)
    img_arr = Image.open(buf).convert("RGB")
    img_arr = img_arr.resize((width, height))
    img_tensor = torch.tensor(np.array(img_arr)).permute(2, 0, 1)
    buf.close()
    plt.close(fig)
    return img_tensor


def torch_img_to_numpy(
    img: Tensor, reverse_colors: bool = False
) -> npt.NDArray[np.uint8]:
    """Converts a torch image (CHW) to a numpy array (HWC). Optionally also converts
    colors.

    Args:
        img (Tensor): Torch image (CHW)
        reverse_colors (bool, optional): Whether to reverse colors (RGB to BGR).
          Defaults to False.

    Returns:
        npt.NDArray[np.uint8]: Numpy image (HWC)
    """
    if img.is_floating_point():
        img = img.mul(255).int()
    img = torch.clamp(img, 0, 255)
    arr = img.detach().cpu().numpy().astype(np.uint8).transpose(1, 2, 0)
    arr = np.ascontiguousarray(arr)
    if reverse_colors:
        arr = cv2.cvtColor(arr, cv2.COLOR_BGR2RGB)
    return arr


def numpy_to_torch_img(img: np.ndarray) -> Tensor:
    """Converts numpy image (HWC) to torch image (CHW)."""
    return torch.from_numpy(img).permute(2, 0, 1)


def preprocess_images(
    imgs: Tensor,
    mean: list[float] | float | None = None,
    std: list[float] | float | None = None,
) -> Tensor:
    """Performs preprocessing on a batch of images.

    Preprocessing includes unnormalizing and converting to uint8.

    Args:
        imgs (Tensor): Batch of images.
        unnormalize_img (bool, optional): Perform unnormalization. Defaults to True.
        normalize_params (dict[str, List[float]] | None, optional):
          Params used for normalization. Must be provided if unnormalize_img is True.
          Defaults to None.

    Returns:
        Tensor: Batch of preprocessed images.
    """
    out_imgs = []
    for i in range(imgs.shape[0]):
        curr_img = imgs[i]
        if mean is not None or std is not None:
            curr_img = unnormalize(curr_img, to_uint8=True, mean=mean, std=std)
        else:
            curr_img = curr_img.to(torch.uint8)

        out_imgs.append(curr_img)

    return torch.stack(out_imgs)


def draw_segmentation_labels(
    img: Tensor,
    label: Tensor,
    alpha: float = 0.4,
    colors: Color | list[Color] | None = None,
) -> Tensor:
    """Draws segmentation labels on an image.

    Args:
        img (Tensor): Image to draw on.
        label (Tensor): Segmentation label.
        alpha (float, optional): Alpha value for blending. Defaults to 0.4.

    Returns:
        Tensor: Image with segmentation labels drawn on.
    """
    masks = label.bool()
    masks = masks.cpu()
    img = img.cpu()
    return draw_segmentation_masks(img, masks, alpha=alpha, colors=colors)


def draw_bounding_box_labels(img: Tensor, label: Tensor, **kwargs) -> Tensor:
    """Draws bounding box labels on an image.

    Args:
        img (Tensor): Image to draw on.
        label (Tensor): Bounding box label. The shape should be (n_instances, 4),
          where the last dimension is (x, y, w, h).
        **kwargs: Additional arguments to pass
          to `torchvision.utils.draw_bounding_boxes`.

    Returns:
        Tensor: Image with bounding box labels drawn on.
    """
    _, H, W = img.shape
    bboxs = box_convert(label, "xywh", "xyxy")
    bboxs[:, 0::2] *= W
    bboxs[:, 1::2] *= H
    return draw_bounding_boxes(img, bboxs, **kwargs)


def draw_keypoint_labels(img: Tensor, label: Tensor, **kwargs) -> Tensor:
    """Draws keypoint labels on an image.

    Args:
        img (Tensor): Image to draw on.
        label (Tensor): Keypoint label. The shape should be (n_instances, 3),
          where the last dimension is (x, y, visibility).
        **kwargs: Additional arguments to pass
          to `torchvision.utils.draw_keypoints`.
    """
    _, H, W = img.shape
    keypoints_unflat = label[:, 1:].reshape(-1, 3)
    keypoints_points = keypoints_unflat[:, :2]
    keypoints_points[:, 0] *= W
    keypoints_points[:, 1] *= H

    n_instances = label.shape[0]
    if n_instances == 0:
        out_keypoints = keypoints_points.reshape((-1, 2)).unsqueeze(0).int()
    else:
        out_keypoints = keypoints_points.reshape((n_instances, -1, 2)).int()

    return draw_keypoints(img, out_keypoints, **kwargs)


def seg_output_to_bool(data: Tensor, binary_threshold: float = 0.5) -> Tensor:
    """Converts seg head output to 2D boolean mask for visualization."""
    masks = torch.empty_like(data, dtype=torch.bool, device=data.device)
    if data.shape[0] == 1:
        classes = torch.sigmoid(data)
        masks[0] = classes >= binary_threshold
    else:
        classes = torch.argmax(data, dim=0)
        for i in range(masks.shape[0]):
            masks[i] = classes == i
    return masks


def unnormalize(
    img: Tensor,
    mean: list[float] | float | None = None,
    std: list[float] | float | None = None,
    to_uint8: bool = False,
) -> Tensor:
    """Unnormalizes an image back to original values, optionally converts it to uint8.

    Args:
        img (Tensor): Image to unnormalize.
        normalize_params (dict[str, List[float]] | None, optional):
          Params used for normalization. If none provided, defaults to imagenet
          normalization. Defaults to None.
        to_uint8 (bool, optional): Whether to convert to uint8. Defaults to False.
    """
    mean = mean or 0
    std = std or 1
    if isinstance(mean, float):
        mean = [mean] * img.shape[0]
    if isinstance(std, float):
        std = [std] * img.shape[0]
    mean_tensor = torch.tensor(mean, device=img.device)
    std_tensor = torch.tensor(std, device=img.device)
    new_mean = -mean_tensor / std_tensor
    new_std = 1 / std_tensor
    out_img = F.normalize(img, mean=new_mean.tolist(), std=new_std.tolist())
    if to_uint8:
        out_img = torch.clamp(out_img.mul(255), 0, 255).to(torch.uint8)
    return out_img


def get_unnormalized_images(cfg: Config, images: Tensor) -> Tensor:
    normalize_params = cfg.train.preprocessing.normalize.params
    mean = std = None
    if cfg.train.preprocessing.normalize.active:
        mean = normalize_params.get("mean", [0.485, 0.456, 0.406])
        std = normalize_params.get("std", [0.229, 0.224, 0.225])
    return preprocess_images(
        images,
        mean=mean,
        std=std,
    )


def get_color(seed: int = 42) -> tuple[int, int, int]:
    """Generates a random color from a seed.

    Args:
        seed (int): Seed to use for the random generator.

    Returns:
        tuple[int, int, int]: Random color.
    """
    seed += 1
    return (seed * 123457) % 255, (seed * 321) % 255, (seed * 654) % 255


# TODO: Support native visualizations
# NOTE: Ignore for now, native visualizations not a priority.
#
# It could be beneficial in the long term to make the visualization more abstract.
# Reason for that is that certain services, e.g. WandB, have their native way
# of visualizing things. So by restricting ourselves to only produce bitmap images
# for logging, we are limiting ourselves in how we can utilize those services.
# (I know we want to leave WandB and I don't know whether mlcloud offers anything
# similar, but it might save us some time in the future).')
#
# The idea would be that every visualizer would not only produce the bitmap
# images, but also some standardized representation of the visualizations.
# This would be sent to the logger, which would then decide how to log it.
# By default, it would log it as a bitmap image, but if we know we are logging
# to (e.g.) WandB, we could use the native WandB visualizations.
# Since we already have to check what logging is being used (to call the correct
# service), it should be somehow easy to implement.
#
# The more specific implementation/protocol could be, that every instance
# of `LuxonisVisualizer` would produce a tuple of
# (bitmap_visualizations, structured_visualizations).
#
# The `bitmap_visualizations` would be one of the following:
# - a single tensor (e.g. image)
#   - in this case, the tensor would be logged as a bitmap image
# - a tuple of two tensors
#   - in this case, the first tensor is considered labels and the second predictions
#   - e.g. GT and predicted segmentation mask
# - a tuple of a tensor and a list of tensors
#   - in this case, the first is considered labels
#     and the second unrelated predictions
# - an iterable of tensors
#   - in this case, the tensors are considered unrelated predictions
#
# The `structured_visualizations` would be have similar format, but  instead of
# tensors, it would consist of some structured data (e.g. dict of lists or something).
# We could even create a validation schema for this to enforce the structure.
# We would then just have to support this new structure in the logger (`LuxonisTracker`).
#
#  TEST:
def combine_visualizations(
    visualization: Tensor | tuple[Tensor, Tensor] | tuple[Tensor, list[Tensor]],
) -> Tensor:
    """Default way of combining multiple visualizations into one final image."""

    def resize_to_match(
        fst: Tensor,
        snd: Tensor,
        *,
        keep_size: Literal["larger", "smaller", "first", "second"] = "larger",
        resize_along: Literal["width", "height", "exact"] = "height",
        keep_aspect_ratio: bool = True,
    ):
        """Resizes two images so they have the same size.

        Resizes two images so they can be concateneted together. It's possible to
        configure how the images are resized.

        Args:
            fst (Tensor[C, H, W]): First image.
            snd (Tensor[C, H, W]): Second image.
            keep_size (Literal["larger", "smaller", "first", "second"], optional):
              Which size to keep. Options are:
                - "larger": Resize the smaller image to match the size of the larger image.
                - "smaller": Resize the larger image to match the size of the smaller image.
                - "first": Resize the second image to match the size of the first image.
                - "second": Resize the first image to match the size of the second image.

              Defaults to "larger".

            resize_along (Literal["width", "height", "exact"], optional):
              Which dimensions to match.
              Options are:
                - "width": Resize images along the width dimension.
                - "height": Resize images along the height dimension.
                - "exact": Resize images to match both width and height dimensions.

              Defaults to "height".

            keep_aspect_ratio (bool, optional):
              Whether to keep the aspect ratio of the images. Only takes effect when
              the "exact" option is selected for the `resize_along` argument.

              Defaults to True.

        Returns:
            tuple[Tensor[C, H, W], Tensor[C, H, W]]: Resized images.
        """
        if resize_along not in ["width", "height", "exact"]:
            raise ValueError(
                "Invalid value for resize_along: {resize_along}. "
                "Valid options are: 'width', 'height', 'exact'."
            )

        _, h1, w1 = fst.shape

        _, h2, w2 = snd.shape

        if keep_size == "larger":
            target_width = max(w1, w2)
            target_height = max(h1, h2)
        elif keep_size == "smaller":
            target_width = min(w1, w2)
            target_height = min(h1, h2)
        elif keep_size == "first":
            target_width = w1
            target_height = h1
        elif keep_size == "second":
            target_width = w2
            target_height = h2
        else:
            raise ValueError(
                f"Invalid value for keep_size: {keep_size}. "
                "Valid options are: 'larger', 'smaller', 'first', 'second'."
            )

        if resize_along == "width":
            target_height = h1 if keep_size in ["first", "larger"] else h2
        elif resize_along == "height":
            target_width = w1 if keep_size in ["first", "larger"] else w2

        if keep_aspect_ratio:
            ar1 = w1 / h1
            ar2 = w2 / h2
            if resize_along == "width" or (
                resize_along == "exact" and target_width / target_height > ar1
            ):
                target_height_fst = int(target_width / ar1)
                target_width_fst = target_width
            else:
                target_width_fst = int(target_height * ar1)
                target_height_fst = target_height
            if resize_along == "width" or (
                resize_along == "exact" and target_width / target_height > ar2
            ):
                target_height_snd = int(target_width / ar2)
                target_width_snd = target_width
            else:
                target_width_snd = int(target_height * ar2)
                target_height_snd = target_height
        else:
            target_width_fst, target_height_fst = target_width, target_height
            target_width_snd, target_height_snd = target_width, target_height

        fst_resized = TF.resize(fst, [target_height_fst, target_width_fst])
        snd_resized = TF.resize(snd, [target_height_snd, target_width_snd])

        return fst_resized, snd_resized

    match visualization:
        case Tensor(data=viz):
            return viz
        case (Tensor(data=viz_labels), Tensor(data=viz_predictions)):
            viz_labels, viz_predictions = resize_to_match(viz_labels, viz_predictions)
            return torch.cat([viz_labels, viz_predictions], dim=2)

        case (Tensor(data=_), [*viz]) if isinstance(viz, list) and all(
            isinstance(v, Tensor) for v in viz
        ):
            raise NotImplementedError(
                "Composition of multiple visualizations not yet supported."
            )
        case _:
            raise ValueError(
                "Visualization should be either a single tensor or a tuple of "
                "two tensors or a tuple of a tensor and a list of tensors."
                f"Got: `{type(visualization)}`."
            )
