trainer:
  accelerator: auto # either "cpu" or "gpu, if "auto" then we cauto-selects best available (string)
  devices: auto # either specify how many devices (int) or which specific devices (list) to use. If auto then automatic selection (int|[int]|string)
  strategy: auto # either one of PL stategies or auto (string)
  num_sanity_val_steps: 2 # number of sanity validation steps performed before training (int)
  profiler: null # use PL profiler for GPU/CPU/RAM utilization analysis (string|null)
  verbose: True # print all intermidiate results in console (bool)

logger:
  project_name: null # name of the project used for logging (string|null)
  project_id: null # id of the project used for logging (relevant if using MLFlow) (string|null)
  run_name: null # name of the run, if empty then auto-generate (string|null)
  run_id: null # id of already create run (relevant if using MLFlow) (string|null)
  save_directory: output # path to the save directory (string)
  is_tensorboard: True # bool if use tensorboard (bool)
  is_wandb: False # bool if use WanDB (bool)
  wandb_entity: null # name of WanDB entity (string|null)
  is_mlflow: False # bool if use MLFlow (bool)
  logged_hyperparams: ["train.epochs", "train.batch_size"] # list of hyperparameters to log (list)

dataset:
  team_id: null # team under which you can find all datasets (string)
  dataset_id: null # id of the dataset (string)
  bucket_type: BucketType.INTERNAL # type of underlying storage (BucketType)
  bucket_storage: BucketStorage.LOCAL # underlying object storage for a bucket (BucketStorage)
  train_view: train # view to use for training (string)
  val_view: val # view to use for validation (string)
  test_view: test # view to use for testing (string)

train:
  preprocessing:
    train_image_size: [256, 256] # image size used for training [height, width] (list)
    keep_aspect_ratio: True # bool if keep aspect ration while resizing (bool)
    train_rgb: True # bool if train on rgb or bgr (bool)
    normalize:
      active: True # bool if use normalization (bool)
      params: # params for normalization (dict|null)
    augmentations: # list of Albumentations augmentations
      # - name: Rotate
      #   params:
      #     limit: 15

  batch_size: 32 # batch size used for trainig (int)
  accumulate_grad_batches: 1 # number of batches for gradient accumulation (int)
  use_weighted_sampler: False # bool if use WeightedRandomSampler for training, only works with classification tasks (bool)
  epochs: 100 # number of training epochs (int)
  num_workers: 2 # number of workers for data loading (int)
  train_metrics_interval: -1 # frequency of computing metrics on train data, -1 if don't perform (int)
  validation_interval: 1 # frequency of computing metrics on validation data (int)
  num_log_images: 4 # maximum number of images to visualize and log (int) 
  skip_last_batch: True # bool if skip last batch while training (bool)
  main_head_index: 0 # index of the head which is used for checkpointing based on best metric (int)
  use_rich_text: True # bool if use rich text for console printing

  callbacks: # callback specific parameters (check PL docs)
    test_on_finish: False # bool if should run test when train loop finishes (bool)
    export_on_finish: False # bool if should run export when train loop finishes - should specify config block (bool)
    use_device_stats_monitor: False # bool if should use device stats monitor during training (bool)
    model_checkpoint:
      save_top_k: 3
    early_stopping:
      active: True
      monitor: val_loss/loss
      mode: min
      patience: 5
      verbose: True

  optimizers: # optimizers specific parameters (check Pytorch docs)
    optimizer:
      name: Adam
      params:
    scheduler:
      name: ConstantLR
      params:
  
  freeze_modules: # defines which modules you want to freeze (not train)
    backbone: False # bool if freeze backbone (bool)
    neck: False # bool if freeze neck (bool)
    heads: [False] # list of bools for specific head freeeze (list[bool])

  losses: # defines weights for losses in multi-head architecture
    log_sub_losses: False # bool if should also log sub-losses (bool)
    weights: [1,1] # list of ints for specific loss weight (list[int])
    # learn_weights: False # bool if weights should be learned (not implemented yet) (bool)

inferer:
  dataset_view: val # view to use for inference (string)
  display: True # bool if should display inference resutls (bool)
  infer_save_directory: null # if this is not null then use this as save directory (string|null)

exporter:
  export_weights: null # path to local weights used for export (string)
  export_save_directory: output_export # path to save directory of exported models (string)
  export_image_size: [256, 256] # image size used for export [height, width] (list)
  export_model_name: model # name of the exported model (string)
  data_type: FP16 # data type used for openVino conversion (string)
  reverse_input_channels: True # bool if reverse input shapes (bool)
  scale_values: [58.395, 57.120, 57.375] # list of scale values (list[int|float])
  mean_values: [123.675, 116.28, 103.53] # list of mean values (list[int|float])
  onnx:
    opset_version: 12 # opset version of onnx used (int)
    dynamic_axes: null # define if dynamic input shapes are used (dict)
  openvino:
    active: False # bool if export to openvino (bool)
  blobconverter:
    active: False # bool if export to blob (bool)
    shaves: 6 # number of shaves used (int)
  s3_upload:
    active: False # bool if upload .ckpt, .onnx and config file to s3 bucket (bool)
    bucket: null # name of the s3 bucket (string)
    upload_directory: null # location of directory for upload (string)

tuner:
  study_name: "test-study" # name of the study (string)
  use_pruner: True # if should use MedianPruner (bool)
  n_trials: 3 # number of trials for each process (int)
  timeout: null # stop study after the given number of seconds (null|int)
  storage:
    active: True # if should use storage to make study persistant (bool)
    type: local # type of storage, "local" or "remote" (string)
  params: # (key, value) pairs for tunning